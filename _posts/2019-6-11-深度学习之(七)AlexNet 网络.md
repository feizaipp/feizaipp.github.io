---
layout:     post
title:      深度学习之(七)AlexNet 网络
#subtitle:  
date:       2019-6-11
author:     feizaipp
header-img: img/post-bg-desk.jpg
catalog: true
tags:
    - DeepLeaning
    - AI
---

> [我的博客](http://feizaipp.github.io)

# 1. 概述
&#160; &#160; &#160; &#160;AlexNet 这个模型的名字来源于论文第一作者的姓名 Alex Krizhevsky 。 AlexNet 使用了 8 层卷积神经网络，并以很大的优势赢得了 ImageNet 2012 图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。

# 2. AlexNet 模型
&#160; &#160; &#160; &#160;AlexNet与LeNet的设计理念非常相似，但也有显著的区别。

&#160; &#160; &#160; &#160;首先，与 LeNet 相比， AlexNet 包含8层变换，其中有 5 层卷积和 2 层全连接隐藏层，以及 1 个全连接输出层。下面我们来详细描述这些层的设计。

&#160; &#160; &#160; &#160;AlexNet 第一层中的卷积窗口形状是 11×11 。因为 ImageNet 中绝大多数图像的高和宽均比 MNIST 图像的高和宽大10倍以上， ImageNet 图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到 5×5 ，之后全采用 3×3 。此外，第一、第二和第五个卷积层之后都使用了窗口形状为 3×3 、步幅为 2 的最大池化层。而且， AlexNet 使用的卷积通道数也大于 LeNet 中的卷积通道数数十倍。紧接着最后一个卷积层的是两个输出个数为 4096 的全连接层。这两个巨大的全连接层带来将近 1 GB 的模型参数。由于早期显存的限制，最早的 AlexNet 使用双数据流的设计使一个 GPU 只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。

&#160; &#160; &#160; &#160;其次， AlexNet 将 sigmoid 激活函数改成了更加简单的 ReLU 激活函数。一方面， ReLU 激活函数的计算更简单，例如它并没有 sigmoid 激活函数中的求幂运算。另一方面， ReLU 激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当 sigmoid 激活函数输出极接近 0 或 1 时，这些区域的梯度几乎为 0 ，从而造成反向传播无法继续更新部分模型参数；而 ReLU 激活函数在正区间的梯度恒为 1 。因此，若模型参数初始化不当， sigmoid 函数可能在正区间得到几乎为 0 的梯度，从而令模型无法得到有效训练。

&#160; &#160; &#160; &#160;再次， AlexNet 通过丢弃法来控制全连接层的模型复杂度。而 LeNet 并没有使用丢弃法。

&#160; &#160; &#160; &#160;最后， AlexNet 引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。