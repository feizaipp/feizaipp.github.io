---
layout:     post
title:      深度学习之(十二)DenseNet 网络
#subtitle:  
date:       2019-7-5
author:     feizaipp
header-img: img/post-bg-desk.jpg
catalog: true
tags:
    - DeepLeaning
    - AI
---

> [我的博客](http://feizaipp.github.io)

# 1. 概述
&#160; &#160; &#160; &#160;ResNet 中的跨层连接设计引申出了数个后续工作。本节我们介绍其中的一个：稠密连接网络(DenseNet) 。 它与ResNet的主要区别如下图所示。

&#160; &#160; &#160; &#160;上图中将部分前后相邻的运算抽象为模块 A 和模块 B 。与 ResNet 的主要区别在于， DenseNet 里模块 B 的输出不是像 ResNet 那样和模块 A 的输出相加，而是在通道维上连结。这样模块A的输出可以直接传入模块 B 后面的层。在这个设计里，模块 A 直接跟模块 B 后面的所有层连接在了一起。这也是它被称为稠密连接的原因。

&#160; &#160; &#160; &#160;DenseNet 的主要构建模块是稠密块(dense block)和过渡层(transition layer)。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。

# 2. 稠密快
&#160; &#160; &#160; &#160;DenseNet 使用了 ResNet 改良版的“批量归一化、激活和卷积”结构，我们首先在 conv_block 函数里实现这个结构。

&#160; &#160; &#160; &#160;稠密块由多个 conv_block 组成，每块使用相同的输出通道数。但在前向计算时，我们将每块的输入和输出在通道维上连结。

&#160; &#160; &#160; &#160;在下面的例子中，我们定义一个有 2 个输出通道数为 10 的卷积块。使用通道数为 3 的输入时，我们会得到通道数为 3+2×10=23 的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率(growth rate)。

# 3. 过渡层
&#160; &#160; &#160; &#160;由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过 1×1 卷积层来减小通道数，并使用步幅为 2 的平均池化层减半高和宽，从而进一步降低模型复杂度。

&#160; &#160; &#160; &#160;对上一个例子中稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半。

# 4. DenseNet 模型
&#160; &#160; &#160; &#160;DenseNet 首先使用同 ResNet 一样的单卷积层和最大池化层。

&#160; &#160; &#160; &#160;类似于 ResNet 接下来使用的 4 个残差块， DenseNet 使用的是 4 个稠密块。同 ResNet 一样，我们可以设置每个稠密块使用多少个卷积层。这里我们设成 4 ，从而与上一节的 ResNet-18 保持一致。稠密块里的卷积层通道数(即增长率)设为 32 ，所以每个稠密块将增加 128 个通道。

&#160; &#160; &#160; &#160;ResNet 里通过步幅为 2 的残差块在每个模块之间减小高和宽。这里我们则使用过渡层来减半高和宽，并减半通道数。

&#160; &#160; &#160; &#160;同 ResNet 一样，最后接上全局池化层和全连接层来输出。
