---
layout:     post
title:      深度学习之(六)LeNet 网络
#subtitle:  
date:       2019-6-11
author:     feizaipp
header-img: img/post-bg-desk.jpg
catalog: true
tags:
    - DeepLeaning
    - AI
---

> [我的博客](http://feizaipp.github.io)

# 1. 概述
&#160; &#160; &#160; &#160;LeNet 这个名字来源于 LeNet 论文的第一作者 Yann LeCun 。 LeNet 展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。

# 2. LeNet 模型
&#160; &#160; &#160; &#160;LeNet 分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。

&#160; &#160; &#160; &#160;卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 5×5 的窗口，并在输出上使用 sigmoid 激活函数。第一个卷积层输出通道数为 6 ，第二个卷积层输出通道数则增加到 16 。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 2×2 ，且步幅为 2 。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。

&#160; &#160; &#160; &#160;卷积层块的输出形状为 (批量大小, 通道, 高, 宽) 。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平 (flatten) 。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含 3 个全连接层。它们的输出个数分别是 120 、 84 和 10 ，其中 10 为输出的类别个数。

# 3. LeNet 网络结构
&#160; &#160; &#160; &#160;LeNet 网络结构如下图所示：

![LeNet 网络结构](/img/LeNet.jpg)

&#160; &#160; &#160; &#160;LeNet 网络实现代码如下：
```
net = nn.Sequential()
net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'), # C1
        nn.MaxPool2D(pool_size=2, strides=2),                                     # S2
        nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),   # C3
        nn.MaxPool2D(pool_size=2, strides=2),                                     # S4
        nn.Dense(120, activation='sigmoid'),                                         # C5
        nn.Dense(84, activation='sigmoid'),                                           # F6
        nn.Dense(10))                                                                           # output
```
&#160; &#160; &#160; &#160;LeNet 网络输入的是尺寸为 32*32 大小的图像。分别经过卷积层、最大池化层、全连接层，最后输出 10 个类别的概率。 LeNet 网络最大的贡献是开创性的使用了卷积层，使卷积神经网络以及后来的深度神经网路得以应用和推广。
